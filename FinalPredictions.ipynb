{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304c8834-3157-41df-9c30-0f074901c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34474593-83d4-4d00-9641-4a12e208603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fef6d8af-248b-4229-89aa-52fc84ee68f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 53172 files belonging to 62 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load the train dataset\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    './Train_osszes',   # Path to your dataset directory\n",
    "    labels='inferred',         # Automatically label based on subdirectory names\n",
    "    label_mode='int',          # 'int' for integer labels, 'categorical' for one-hot labels\n",
    "    color_mode=\"grayscale\",    # only one channel\n",
    "    batch_size=32,             # Number of images per batch\n",
    "    image_size=(128, 128),     # Resize all images to this size\n",
    "    shuffle=True,             # Shuffle the dataset\n",
    "    seed=42                    # Optional seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1d4ff15-47d8-4892-9f06-0757aced22b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7100 files.\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    './TestData',   # Path to your dataset directory\n",
    "    labels=None,         # Automatically label based on subdirectory names\n",
    "    color_mode=\"grayscale\",    # only one channel\n",
    "    batch_size=32,             # Number of images per batch\n",
    "    image_size=(128, 128),     # Resize all images to this size\n",
    "    shuffle=False,             # Shuffle the dataset\n",
    "    seed=42                    # Optional seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8afdfb04-5be9-4c14-8078-9be46aef7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda x, y: (tf.image.resize(x, [image_shape, image_shape]), y))\n",
    "test_dataset = test_dataset.map(lambda x: (tf.image.resize(x, [image_shape, image_shape])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b619432f-15e7-4fe0-97d4-c2afeec45c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# Convert dataset to numpy arrays\n",
    "def dataset_to_numpy(dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image_batch, label_batch in dataset:\n",
    "        images.append(image_batch.numpy())\n",
    "        labels.append(label_batch.numpy())\n",
    "    \n",
    "    images = np.concatenate(images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d02159a0-6c23-4fd1-b1e3-46514b546e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, labels = dataset_to_numpy(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "841d5016-a1ec-41e5-b9a1-cdaad78c06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset_to_numpy(dataset):\n",
    "    images = []\n",
    "    for image_batch in dataset:\n",
    "        images.append(image_batch.numpy())\n",
    "    \n",
    "    images = np.concatenate(images, axis=0)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "912aa070-5a40-4ed5-8206-e1294b9f5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_dataset_to_numpy(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb510726-af53-4975-90af-56483c26a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from keras.layers import Input, Conv2D, MaxPool2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define model creation function\n",
    "def create_model(print_summary=False): \n",
    "    model = Sequential([\n",
    "        Input(shape=(image_shape, image_shape, 1)),\n",
    "        Conv2D(32, 3, activation='relu'),\n",
    "        MaxPool2D((2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(64, 3, activation='relu'),\n",
    "        MaxPool2D((2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        BatchNormalization(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(62, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    if(print_summary): model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bf8b1c2-6d54-439c-9e38-cd0f19513a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 32ms/step - accuracy: 0.5650 - loss: 1.7165 - learning_rate: 0.0010\n",
      "Epoch 2/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.8288 - loss: 0.5066 - learning_rate: 0.0010\n",
      "Epoch 3/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.8510 - loss: 0.4086 - learning_rate: 0.0010\n",
      "Epoch 4/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.8662 - loss: 0.3506 - learning_rate: 0.0010\n",
      "Epoch 5/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.8761 - loss: 0.3197 - learning_rate: 0.0010\n",
      "Epoch 6/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 32ms/step - accuracy: 0.8824 - loss: 0.2981 - learning_rate: 0.0010\n",
      "Epoch 7/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.8930 - loss: 0.2725 - learning_rate: 0.0010\n",
      "Epoch 8/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 31ms/step - accuracy: 0.8956 - loss: 0.2591 - learning_rate: 0.0010\n",
      "Epoch 9/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 32ms/step - accuracy: 0.9036 - loss: 0.2485 - learning_rate: 0.0010\n",
      "Epoch 10/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 31ms/step - accuracy: 0.9042 - loss: 0.2353 - learning_rate: 0.0010\n",
      "Epoch 11/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 32ms/step - accuracy: 0.9098 - loss: 0.2269 - learning_rate: 0.0010\n",
      "Epoch 12/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 32ms/step - accuracy: 0.9119 - loss: 0.2219 - learning_rate: 0.0010\n",
      "Epoch 13/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - accuracy: 0.9109 - loss: 0.2186 - learning_rate: 0.0010\n",
      "Epoch 14/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9127 - loss: 0.2137 - learning_rate: 0.0010\n",
      "Epoch 15/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9186 - loss: 0.2017 - learning_rate: 0.0010\n",
      "Epoch 16/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - accuracy: 0.9205 - loss: 0.1953 - learning_rate: 0.0010\n",
      "Epoch 17/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9216 - loss: 0.1983 - learning_rate: 0.0010\n",
      "Epoch 18/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - accuracy: 0.9271 - loss: 0.1806 - learning_rate: 0.0010\n",
      "Epoch 19/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - accuracy: 0.9237 - loss: 0.1894 - learning_rate: 0.0010\n",
      "Epoch 20/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - accuracy: 0.9228 - loss: 0.1897 - learning_rate: 0.0010\n",
      "Epoch 21/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9279 - loss: 0.1778 - learning_rate: 0.0010\n",
      "Epoch 22/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9298 - loss: 0.1768 - learning_rate: 0.0010\n",
      "Epoch 23/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 35ms/step - accuracy: 0.9275 - loss: 0.1784 - learning_rate: 0.0010\n",
      "Epoch 24/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 35ms/step - accuracy: 0.9314 - loss: 0.1690 - learning_rate: 0.0010\n",
      "Epoch 25/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 37ms/step - accuracy: 0.9339 - loss: 0.1647 - learning_rate: 0.0010\n",
      "Epoch 26/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - accuracy: 0.9336 - loss: 0.1635 - learning_rate: 0.0010\n",
      "Epoch 27/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9354 - loss: 0.1593 - learning_rate: 0.0010\n",
      "Epoch 28/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9344 - loss: 0.1603 - learning_rate: 0.0010\n",
      "Epoch 29/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9362 - loss: 0.1573 - learning_rate: 0.0010\n",
      "Epoch 30/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 33ms/step - accuracy: 0.9374 - loss: 0.1522 - learning_rate: 0.0010\n",
      "Epoch 31/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9392 - loss: 0.1498 - learning_rate: 0.0010\n",
      "Epoch 32/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9395 - loss: 0.1500 - learning_rate: 0.0010\n",
      "Epoch 33/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9394 - loss: 0.1479 - learning_rate: 0.0010\n",
      "Epoch 34/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9394 - loss: 0.1481 - learning_rate: 0.0010\n",
      "Epoch 35/35\n",
      "\u001b[1m831/831\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 33ms/step - accuracy: 0.9397 - loss: 0.1495 - learning_rate: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2061c4c3da0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint\n",
    "model = create_model()\n",
    "RLP = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "model.fit(train_images, labels, epochs=35, batch_size=64, callbacks=[RLP])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76c3eb3b-8580-4878-96c7-20649a93699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4eaa763f-b916-40b9-b758-103b7e6de036",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = np.argmax(pred, axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1b8b6eb-ee36-4923-88dd-840b978b3cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ae18772-38a1-4100-88aa-82ce2b127612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_directory = './TestData'\n",
    "file_names = [f for f in os.listdir(image_directory) if f.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "222d2d82-a718-4807-8afb-002cf505cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# CSV fájl létrehozása és írása\n",
    "with open('OCRpredictions.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=';')  # Használj ';' elválasztót\n",
    "    writer.writerow(['class', 'TestImage'])  # Fejléc\n",
    "    for pred, name in zip(predicted_values, file_names):\n",
    "        writer.writerow([pred, name])  # Először osztály, utána fájlnév\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "014f0edc-2680-458b-8f6a-a2d7ccdbdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd2=pd.read_csv('./OCRpredictions.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f30e3ad5-19c9-431a-a28c-c0a949caa630",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(pd2)!=7100:\n",
    "    print('Error! Number of predictions should be 7100!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d231221c-a1d8-4b3a-9361-9787b03d298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pd2['class'][0]>62:\n",
    "    print('Error!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d814068e-b375-45c6-88e9-195a4c4548c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pd2['class'].max()>62 or pd2['class'].min()<1:\n",
    "    print('Error! All class labels should be between 1 and 62! First class label is not zero!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7d983cf7-1768-40ec-b808-bd9078e0d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pd2['TestImage'][0]!='Test0001.png':\n",
    "    print('Error! First image should be the Test0001.png.')\n",
    "if pd2['TestImage'][7100-1]!='Test7100.png':\n",
    "    print('Error! Last image should be the Test7100.png.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ae2c1-d118-4edd-a129-a66dfcac8eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51399b-7100-4ab5-9974-b7801dce93c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e86a03-02c9-4be0-a835-3ec389b5637d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62cd27-7b72-427d-bdcc-dfe61183d246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
